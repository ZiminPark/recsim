{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RecSim: Developing an Agent.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZiminPark/recsim/blob/master/recsim/colab/RecSim_Developing_an_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtBSsFuoaaNv"
      },
      "source": [
        "# Developing an Agent\n",
        "\n",
        "마지막 퍼즐 Agent 개발로 넘어가 보자. 이번 튜토리얼에서 두 가지를 다룰 것이다.\n",
        "\n",
        "* basics : RecSim에서 어떤 데이터가 agent에 들어가는지, 어떤 걸 return 받기 원하는지\n",
        "\n",
        "* design: 어떤 피쳐가 RecSim에서 agents를 개발하는데 필요한지."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVQshsAMdYai"
      },
      "source": [
        "# Basics\n",
        "\n",
        "\n",
        "<p align=\"center\"><img width=\"50%\" src=\"https://github.com/google-research/recsim/blob/master/recsim/colab/figures/recsim_architecture_agent_centered.png?raw=true\" /></p>\n",
        "\n",
        "Agent는 다음을 consume해야 한다.\n",
        "\n",
        "* 유저 state에 대한 observations\n",
        "* 추천에 대한 유저의 반응 observations\n",
        "* 가능한 documents와 documents 피쳐 벡터.\n",
        "\n",
        "return으로 agent는 $K$개의 추천을 만들고 user's choice와 transition model에 넘겨준다.\n",
        "\n",
        "RecSim's agent API를 설명하기 위해 simple bandit agent for RecSim's *interest exploration* environment을 사용해보자."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1drgXe9CRqH"
      },
      "source": [
        "*interest exploration*은 clustered bandit problem을 말한다. 세상은 topics로 cluster될 수 있는 documents로 구성되어 있고 유저도 cluster될 수 있다고 가정한다.\n",
        "\n",
        "유저의 document에 대한 affinity는 document의 퀄리티 + 유저의 주제에 대한 affinity다.\n",
        "\n",
        "이런 상황은 클릭율이 높은 documents를 높은 순위로 책정하여 근시안적인 정책을 만들어낸다. 이는 suboptimal policy도 수렴하게 되기 때문에 active exploration이 필요하다.\n",
        "\n",
        "method를 각각 나눠보고 나중에 합치자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BHEmh8BUhR_"
      },
      "source": [
        "!pip install --upgrade --no-cache-dir recsim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlMUGf8CdX6D"
      },
      "source": [
        "import functools\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "from recsim import agent\n",
        "from recsim import document\n",
        "from recsim import user\n",
        "from recsim.choice_model import MultinomialLogitChoiceModel\n",
        "from recsim.simulator import environment\n",
        "from recsim.simulator import recsim_gym\n",
        "from recsim.simulator import runner_lib\n",
        "from recsim.environments import interest_exploration"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tismgekhub0Z"
      },
      "source": [
        "env는 관심사가 아니기 때문에 주어지는 걸 사용하자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6Z9GmFDEYjD"
      },
      "source": [
        "env_config = {'slate_size': 2,\n",
        "              'seed': 0,\n",
        "              'num_candidates': 15,\n",
        "              'resample_documents': True}\n",
        "ie_environment = interest_exploration.create_environment(env_config)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weXPwsYAFV6W"
      },
      "source": [
        "initial_observation = ie_environment.reset()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSDtDdzSF45O"
      },
      "source": [
        "## Observations\n",
        "\n",
        "RecSim의 observation은 dict이고 key가 3가지 있다:\n",
        "* 'user', 위 그림에서 'User Observable Features'를 나타낸다.\n",
        "* 'doc', 추천할 수 있는 Document와 그 피쳐('Document Observable Features'),\n",
        "* 'response', 마지막 추천에 대한 유저의 반응('User Response'). \n",
        "\n",
        "이번 environment에서는 유저의 observable features를 구현하지 않았다. 그래서 이 필드는 계속 empty이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRJd0MWaGale",
        "outputId": "c8bd1a22-ec25-4bea-8825-783c11d97289",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('User Observable Features')\n",
        "print(initial_observation['user'])\n",
        "print('User Response')\n",
        "print(initial_observation['response'])\n",
        "print('Document Observable Features')\n",
        "for doc_id, doc_features in initial_observation['doc'].items():\n",
        "  print('ID:', doc_id, 'features:', doc_features)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "User Observable Features\n",
            "[]\n",
            "User Response\n",
            "None\n",
            "Document Observable Features\n",
            "ID: 15 features: {'quality': array(1.22720163), 'cluster_id': 1}\n",
            "ID: 16 features: {'quality': array(1.29258489), 'cluster_id': 1}\n",
            "ID: 17 features: {'quality': array(1.23977078), 'cluster_id': 1}\n",
            "ID: 18 features: {'quality': array(1.46045555), 'cluster_id': 1}\n",
            "ID: 19 features: {'quality': array(2.10233425), 'cluster_id': 0}\n",
            "ID: 20 features: {'quality': array(1.09572905), 'cluster_id': 1}\n",
            "ID: 21 features: {'quality': array(2.37256963), 'cluster_id': 0}\n",
            "ID: 22 features: {'quality': array(1.34928002), 'cluster_id': 1}\n",
            "ID: 23 features: {'quality': array(1.00670188), 'cluster_id': 1}\n",
            "ID: 24 features: {'quality': array(1.20448562), 'cluster_id': 1}\n",
            "ID: 25 features: {'quality': array(2.18351159), 'cluster_id': 0}\n",
            "ID: 26 features: {'quality': array(1.19411585), 'cluster_id': 1}\n",
            "ID: 27 features: {'quality': array(1.03514646), 'cluster_id': 1}\n",
            "ID: 28 features: {'quality': array(2.29592623), 'cluster_id': 0}\n",
            "ID: 29 features: {'quality': array(2.05936556), 'cluster_id': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOvbXmVQF371",
        "outputId": "25516ad1-b7b2-4622-bce8-06cc9db2b644",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('Document observation space')\n",
        "for key, space in ie_environment.observation_space['doc'].spaces.items():\n",
        "  print(key, ':', space)\n",
        "print('Response observation space')\n",
        "print(ie_environment.observation_space['response'])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Document observation space\n",
            "15 : Dict(cluster_id:Discrete(2), quality:Box(0.0, inf, (), float32))\n",
            "16 : Dict(cluster_id:Discrete(2), quality:Box(0.0, inf, (), float32))\n",
            "17 : Dict(cluster_id:Discrete(2), quality:Box(0.0, inf, (), float32))\n",
            "18 : Dict(cluster_id:Discrete(2), quality:Box(0.0, inf, (), float32))\n",
            "19 : Dict(cluster_id:Discrete(2), quality:Box(0.0, inf, (), float32))\n",
            "20 : Dict(cluster_id:Discrete(2), quality:Box(0.0, inf, (), float32))\n",
            "21 : Dict(cluster_id:Discrete(2), quality:Box(0.0, inf, (), float32))\n",
            "22 : Dict(cluster_id:Discrete(2), quality:Box(0.0, inf, (), float32))\n",
            "23 : Dict(cluster_id:Discrete(2), quality:Box(0.0, inf, (), float32))\n",
            "24 : Dict(cluster_id:Discrete(2), quality:Box(0.0, inf, (), float32))\n",
            "25 : Dict(cluster_id:Discrete(2), quality:Box(0.0, inf, (), float32))\n",
            "26 : Dict(cluster_id:Discrete(2), quality:Box(0.0, inf, (), float32))\n",
            "27 : Dict(cluster_id:Discrete(2), quality:Box(0.0, inf, (), float32))\n",
            "28 : Dict(cluster_id:Discrete(2), quality:Box(0.0, inf, (), float32))\n",
            "29 : Dict(cluster_id:Discrete(2), quality:Box(0.0, inf, (), float32))\n",
            "Response observation space\n",
            "Tuple(Dict(click:Discrete(2), cluster_id:Discrete(2), quality:Box(0.0, inf, (), float32)), Dict(click:Discrete(2), cluster_id:Discrete(2), quality:Box(0.0, inf, (), float32)))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "St4Kr6vYbmCb"
      },
      "source": [
        "## Slates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GARdpz7tEN83",
        "outputId": "bb513bd1-0fed-40c9-e058-c8f9ecea9d98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "slate = [0, 1]\n",
        "for slate_doc in slate:\n",
        "  print(list(initial_observation['doc'].items())[slate_doc])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('15', {'quality': array(1.22720163), 'cluster_id': 1})\n",
            "('16', {'quality': array(1.29258489), 'cluster_id': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_k6EaxpdOSE",
        "outputId": "4e2cf081-ee1b-467a-b544-58f965283238",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ie_environment.action_space"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultiDiscrete([15 15])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFbqcvKCiIdY"
      },
      "source": [
        "첫 번째 slate가 주어지면 simulator은 env에서 step하고 새로운 observation과 reward를 만든다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5htT89RiGdO"
      },
      "source": [
        "observation, reward, done, _ = ie_environment.step(slate)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xd5q0q2qdeX5"
      },
      "source": [
        "The main job of the agent is to produce a valid slate for each step of the simulation. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K011d6qbee6J"
      },
      "source": [
        "## A trivial agent\n",
        "\n",
        "- 기본적인 수준에서 agent는 step-function만 구현하는 것으로도 충분하다.\n",
        "- 일단 첫 K개를 추천하는 agent를 만들어보자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdAGgbcped1J"
      },
      "source": [
        "from recsim.agent import AbstractEpisodicRecommenderAgent"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbSIEw1ngq0L"
      },
      "source": [
        "- RecSim agent는 *AbstractEpisodicRecommenderAgent*를 상속 받는다. \n",
        "\n",
        "- observation_space 와 action_space가 init에 필요하다. \n",
        "\n",
        "- 이것들을 이용하여 environment가 agent 운영의 전제조건을 충족하는지 검증할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4g5xIwxdb4Z"
      },
      "source": [
        "class StaticAgent(AbstractEpisodicRecommenderAgent):\n",
        "  def __init__(self, observation_space, action_space):\n",
        "    # Check if document corpus is large enough.\n",
        "    if len(observation_space['doc'].spaces) < len(action_space.nvec):\n",
        "      raise RuntimeError('Slate size larger than size of the corpus.')\n",
        "    super(StaticAgent, self).__init__(action_space)\n",
        "\n",
        "  def step(self, reward, observation):\n",
        "    print(observation)\n",
        "    return list(range(self._slate_size))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSCkNfBlsI0E"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.compat.v1.disable_eager_execution()"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKV4mRcgiorx",
        "outputId": "c0632704-d271-4fd4-fec0-e8fb3ac1d276",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def create_agent(sess, environment, eval_mode, summary_writer=None):\n",
        "  return StaticAgent(environment.observation_space, environment.action_space)\n",
        "\n",
        "tmp_base_dir = '/tmp/recsim/'\n",
        "\n",
        "runner = runner_lib.EvalRunner(\n",
        "  base_dir=tmp_base_dir,\n",
        "  create_agent_fn=create_agent,\n",
        "  env=ie_environment,\n",
        "  max_eval_episodes=1,\n",
        "  max_steps_per_episode=5,\n",
        "  test_mode=True)\n",
        "\n",
        "# We won't run this, but we totally could\n",
        "# runner.run_experiment()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:max_eval_episodes = 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:max_eval_episodes = 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:max_steps_per_episode = 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:max_steps_per_episode = 5\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jae7rcS8nG5S"
      },
      "source": [
        "# Design: Hierarchical Agent Layers \n",
        "\n",
        "- basic을 만들었으니 좀 더 어려운 것에 도전하고 싶다. \n",
        "\n",
        "- bandit algorithm을 돌려볼 수 있겠다. 유저의 intent가 observable하다고 가정하고 policy을 만들어보자.\n",
        "\n",
        "- 전처리, 추천, 후처리 등 hierarchical한 추천시스템 구조를 설명하는 거 같은데 음... 어떤 맥락에서 이런 이야기가 나왔는지는 원문을 다시보자.\n",
        "\n",
        "The way this problem is set up, a natural heuristic presents itself. We can run a bandit algorithm to reveal the average engagement of a user with each cluster of documents. That is, each cluster becomes an arm. Once the algorithm has chosen a cluster, we serve take the highest quality video from that cluster. This is a metaphor for a situation that occurs often in recommender systems that serve as a front end to multiple (sub-)products: within each session, the user will interact with the recommender with some intent in mind, that is, to realize some task that can be fulfilled by one of the possible sub-products. Sometimes, the user will issue an explicit query (e.g., enter search terms), which effectively makes that intent observable up ot query interpretation uncertainty. Most often, however, the intent will be latent -- the user will reveal it indirectly by chosing among a set of items from the slate. We assume that had the intent been observable, a product-specific policy would be available to fulfill it.\n",
        "\n",
        "This set-up captures some typical features of practical recommender systems -- they tend to very hierarchical, often very heuristic due to the complexity of the environment they operate in, and also very idiosyncratic to the task at hand. For this reason, RecSim's approach to agent engineering is very modular. Instead of providing a wide array of agents, we provide an easily extendable set of agent building blocks, called Agent Layers, which could be combined into hierarchies to create more complex agents.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8mhcm9jEHKv"
      },
      "source": [
        "## Hierarchical agent layers\n",
        "![Hierarchical agent architecture](https://github.com/google-research/recsim/blob/master/recsim/colab/figures/agent_architecture.png?raw=true)\n",
        "\n",
        "A hierarchical agent layer does not materialize a slate of documents, but relies on one or more base agents to do so. The hierarchical agent architecture in RecSim can roughly be described follows:\n",
        "* a hierarchical agent layer receives an observationand reward from the environment; it preprocesses the raw observation and passes it to one or more base agents.\n",
        "* Each base agent outputs either a slate or an abstract action (depending on the use case), which is then post-processed by the layer to create/output the slate (concrete action). \n",
        "\n",
        "Hierarchical layers are recursively stackable in a fashion similar to Keras layers. Hierarchical  layers  are  defined  by  their  pre-  and  post-processing functions and can play many roles dependinghow these are implemented. For example, a layer can beused as a pure feature injector — it can extract some feature from the (history of) observations and pass it to the base agent, while keeping the post-processing function vacuous. This allows decoupling of feature- and agent-engineering. Various regularizers can be implemented in a similar fashion by modifying the reward. Layers may also be stateful and dynamic, as the pre- or post-processing functions may implement parameter updates or learning mechanisms. \n",
        "\n",
        "We will not discuss how to implement these layers here (the reader is referred to examples in the *layers/* directory), rather, we will show their usage and benefits. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MB4VQFzjqKAq"
      },
      "source": [
        "## ClusterClickStats\n",
        "\n",
        "Recall that the *Interest Exploration* provides clicks as feedback, but does not keep track of cumulative click counts or impression counts. Since maintaining such statistics is generally useful, we provide an agent layer that does exactly that. That is, it monitors the stream of responses and retains the number of clicks and impressions from each cluster. The precondition is that the response space has a key 'click', as well as 'cluster_id'. If this is met, than the layer can be used with any environment/agent. Let's see how this works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmKdauxrEGzY"
      },
      "source": [
        "from recsim.agents.layers.cluster_click_statistics import ClusterClickStatsLayer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2lvoE93ma8g"
      },
      "source": [
        "\n",
        "A hierarchical agent layer is instantiated in a smilar way to usual agents, except that it takes in a constructor for a base agent, that is, an agent whose abstract action it can interpret. In the case of cluster click stats, it will not do any post-processing of the abstract action, that is, it simply relays the action of the base agent to the environment. This implies that the base agent will need to provide a full slate. \n",
        "\n",
        "Once instantiated, the cluster click stats layer will inject a sufficient statistic to the base agent's observation space containing clicks and impressions. Thus, the combination of both will behave like as if the base agent had an additional field in its observation space. We showcase this using our StaticAgent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EMysDE43nn8",
        "outputId": "810376cb-cf1e-42d8-9e2c-2a0e5bf4a8c2",
        "colab": {
          "height": 70
        }
      },
      "source": [
        "static_agent = StaticAgent(ie_environment.observation_space,\n",
        "                           ie_environment.action_space)\n",
        "static_agent.step(reward, observation)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'user': array([], dtype=float64), 'doc': {'30': {'quality': 2.489224450301943, 'cluster_id': 0}, '31': {'quality': 2.125926607579561, 'cluster_id': 0}, '32': {'quality': 1.27448138607991, 'cluster_id': 1}, '33': {'quality': 1.2179911236932994, 'cluster_id': 1}, '34': {'quality': 1.177703750911228, 'cluster_id': 1}, '35': {'quality': 2.079489146813576, 'cluster_id': 0}, '36': {'quality': 1.1416765236282371, 'cluster_id': 1}, '37': {'quality': 1.2052916542615082, 'cluster_id': 1}, '38': {'quality': 1.2424683972006194, 'cluster_id': 1}, '39': {'quality': 1.8727966807396805, 'cluster_id': 0}, '40': {'quality': 1.1964488835024119, 'cluster_id': 1}, '41': {'quality': 1.282540205315461, 'cluster_id': 1}, '42': {'quality': 2.015585394934561, 'cluster_id': 0}, '43': {'quality': 2.464004827721051, 'cluster_id': 0}, '44': {'quality': 1.33980633202097, 'cluster_id': 1}}, 'response': ({'click': 0, 'quality': 1.2272016322975663, 'cluster_id': 1}, {'click': 0, 'quality': 1.2925848895378007, 'cluster_id': 1})}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5H7wzqfD27Em",
        "outputId": "72fb909c-e8d9-4e5b-8e0d-856e8bde0e3a",
        "colab": {
          "height": 70
        }
      },
      "source": [
        "cluster_static_agent = ClusterClickStatsLayer(StaticAgent,\n",
        "                                              ie_environment.observation_space,\n",
        "                                              ie_environment.action_space)\n",
        "cluster_static_agent.step(reward, observation)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'user': {'raw_observation': array([], dtype=float64), 'sufficient_statistics': {'impression_count': array([0, 2]), 'click_count': array([0, 0])}}, 'doc': {'30': {'quality': 2.489224450301943, 'cluster_id': 0}, '31': {'quality': 2.125926607579561, 'cluster_id': 0}, '32': {'quality': 1.27448138607991, 'cluster_id': 1}, '33': {'quality': 1.2179911236932994, 'cluster_id': 1}, '34': {'quality': 1.177703750911228, 'cluster_id': 1}, '35': {'quality': 2.079489146813576, 'cluster_id': 0}, '36': {'quality': 1.1416765236282371, 'cluster_id': 1}, '37': {'quality': 1.2052916542615082, 'cluster_id': 1}, '38': {'quality': 1.2424683972006194, 'cluster_id': 1}, '39': {'quality': 1.8727966807396805, 'cluster_id': 0}, '40': {'quality': 1.1964488835024119, 'cluster_id': 1}, '41': {'quality': 1.282540205315461, 'cluster_id': 1}, '42': {'quality': 2.015585394934561, 'cluster_id': 0}, '43': {'quality': 2.464004827721051, 'cluster_id': 0}, '44': {'quality': 1.33980633202097, 'cluster_id': 1}}, 'response': ({'click': 0, 'quality': 1.2272016322975663, 'cluster_id': 1}, {'click': 0, 'quality': 1.2925848895378007, 'cluster_id': 1})}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeyrpnNI3onL"
      },
      "source": [
        "Observe how the 'user' field of the observation dictionary (as printed from within the static agent's step function) now has a new key 'sufficient_statistics', whereas the old user observation (which is vacuous) went under the 'raw_observation' key. This is done to avoid naming conflicts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6FSXE_eqCMP"
      },
      "source": [
        "## AbstractClickBandit\n",
        "\n",
        "The ClusterClickStats layer takes care of computing the necessary sufficient statistics for exploration. To implement the actual bandit policy, RecSim offers an abstract bandit layer implementation. The *AbstractClickBandit* takes as input a list of base agents, which it treats as arms. It will then utilize one of a a few implemented bandit policies (UCB1, KL-UCB, ThompsonSampling) to mix the policies in a way that achieves sub-linear regret relative to the best policy (which is apriori unknown), subject to certain assumptions about the environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0vDml6hqXSS"
      },
      "source": [
        "from recsim.agents.layers.abstract_click_bandit import AbstractClickBanditLayer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYA94M_puOs1"
      },
      "source": [
        "To instantiate an abstract bandit, we must present a list of base agents. In our case, we will have one base agent for each cluster. That agent simply retrieves the documents of that cluster from the corpus and sorts them according to perceived quality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRRAhUTtun-S"
      },
      "source": [
        "class GreedyClusterAgent(agent.AbstractEpisodicRecommenderAgent):\n",
        "  \"\"\"Simple agent sorting all documents of a topic according to quality.\"\"\"\n",
        "\n",
        "  def __init__(self, observation_space, action_space, cluster_id, **kwargs):\n",
        "    del observation_space\n",
        "    super(GreedyClusterAgent, self).__init__(action_space)\n",
        "    self._cluster_id = cluster_id\n",
        "\n",
        "  def step(self, reward, observation):\n",
        "    del reward\n",
        "    my_docs = []\n",
        "    my_doc_quality = []\n",
        "    for i, doc in enumerate(observation['doc'].values()):\n",
        "      if doc['cluster_id'] == self._cluster_id:\n",
        "        my_docs.append(i)\n",
        "        my_doc_quality.append(doc['quality'])\n",
        "    if not bool(my_docs):\n",
        "      return []\n",
        "    sorted_indices = np.argsort(my_doc_quality)[::-1]\n",
        "    return list(np.array(my_docs)[sorted_indices])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph9XmH_1usdR"
      },
      "source": [
        "We will now instantiate one GreedyClusterAgent for each cluster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQ7DO1yqus_f"
      },
      "source": [
        "  num_topics = list(ie_environment.observation_space.spaces['doc']\n",
        "                    .spaces.values())[0].spaces['cluster_id'].n\n",
        "  base_agent_ctors = [\n",
        "      functools.partial(GreedyClusterAgent, cluster_id=i)\n",
        "      for i in range(num_topics)\n",
        "  ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lld6Y3KZvWVy"
      },
      "source": [
        "We can now instantiate our cluster bandit as a combination of ClusterClickStats, AbstractClickBandit, and GreedyClusterAgent:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrgbdGUSvVcj"
      },
      "source": [
        "bandit_ctor = functools.partial(AbstractClickBanditLayer,\n",
        "                                arm_base_agent_ctors=base_agent_ctors)\n",
        "cluster_bandit = ClusterClickStatsLayer(bandit_ctor,\n",
        "                                        ie_environment.observation_space,\n",
        "                                        ie_environment.action_space)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_HoVSVhww-n"
      },
      "source": [
        "Our ClusterBandit is ready to use!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LVPU3ukw06u",
        "outputId": "4f9e9b4e-8ea0-4ca8-82d7-c2e16c27f9e0",
        "colab": {
          "height": 66
        }
      },
      "source": [
        "observation0 = ie_environment.reset()\n",
        "slate = cluster_bandit.begin_episode(observation0)\n",
        "print(\"Cluster bandit slate 0:\")\n",
        "doc_list = list(observation0['doc'].values())\n",
        "for doc_position in slate:\n",
        "  print(doc_list[doc_position])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cluster bandit slate 0:\n",
            "{'quality': 1.4686875120276195, 'cluster_id': 1}\n",
            "{'quality': 1.4226918183479484, 'cluster_id': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}