{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RecSim: Overview.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZiminPark/recsim/blob/master/recsim/colab/RecSim_Overview.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkMTOmHpHKE8"
      },
      "source": [
        "# Running RecSim\n",
        "In this Colab we explore how to train and evaulate an agent within RecSim using the provided environments and clarify some basic concepts along the way. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VX8egcjA2A3e"
      },
      "source": [
        "\n",
        "# RecSim at a Glance\n",
        "RecSim is a configurable platform for simulating a recommendation system environment in which a recommender agent interacts with a corpus of documents (or recommendable items) and a set of users, in a natural but abstract fashion, to support the development of new recommendation algorithms.\n",
        "At its core, a RecSim simulation consists of running the following event loop for some fixed number of sessions (episodes):\n",
        "\n",
        "\n",
        "\n",
        "![RecSim at a glance](https://github.com/google-research/recsim/blob/master/recsim/colab/figures/recsim_at_a_glance.png?raw=true)\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "for episode in [1,...,number_of_episodes]:\n",
        "  user = sample_user()\n",
        "  recommended_slate = null\n",
        "  while session_not_over:\n",
        "    user_response = user_responds_to_recommendation(recommended_slate)\n",
        "    available_documents = sample_documents_from_database()\n",
        "    recommended_slate = agent_step(available_documents, user_response)\n",
        "```\n",
        "\n",
        "The document database (document model), user model, and recommender agent each have various internal components, and we will discuss how to design and implement them in later colabs ([Developing an Environment](RecSim_Developing_an_Environment.ipynb), [Developing an Agent](RecSim_Developing_an_Agent.ipynb)). For now, we will see how to set up one of the ready-made environments that ship with RecSim in order to run a simulation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaWqFNX_E6vt"
      },
      "source": [
        "# @title Install\n",
        "!pip install --upgrade --no-cache-dir recsim\n",
        "!pip install -q tf-nightly-2.0-preview\n",
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "CiB3XD9yy0Ne"
      },
      "source": [
        "#@title Importing generics\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipOB1-sqIOGC"
      },
      "source": [
        "In RecSim, a user model and a document model are packaged together within an OpenAI Gym-style environment. In this tutorial, we will use the \"Interest Evolution\" environment used in [Ie et al.](https://arxiv.org/abs/1905.12767), as well as a full Slate-Q agent also described therein. Both come ready to use with RecSim. We import the environment from recsim.environments. Agents are found in recsim.agents. Finally, we need to import runner_lib from recsim.simulator, which executes the loop outlined above. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Smj_NRctHZUy"
      },
      "source": [
        "#@title Importing RecSim components \n",
        "from recsim.environments import interest_evolution\n",
        "from recsim.agents import full_slate_q_agent\n",
        "from recsim.simulator import runner_lib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq7smdtan_r3"
      },
      "source": [
        "#Creating an Agent\n",
        "\n",
        "Similarly to Dopamine, a RecSim experiment runner (simulator) consumes an environment creation function and an agent creation function. These functions are responsible for setting up the environment/agent based on external parameters. The interest evolution environment already comes with a creation function, so we will limit our attention to the agent.\n",
        "\n",
        "A create_agent function takes a tensorflow session, environment object, a training/eval flag and (optionally) a Tensorflow summary writer, which are passed to the agent for in-agent training statistics in Tensorboard (more on that below). In the case of full Slate-Q, we just need to extract the action and observation spaces from the environment and pass them to the agent constructor. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdS14uuOn7ZE"
      },
      "source": [
        "def create_agent(sess, environment, eval_mode, summary_writer=None):\n",
        "  kwargs = {\n",
        "      'observation_space': environment.observation_space,\n",
        "      'action_space': environment.action_space,\n",
        "      'summary_writer': summary_writer,\n",
        "      'eval_mode': eval_mode,\n",
        "  }\n",
        "  return full_slate_q_agent.FullSlateQAgent(sess, **kwargs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ekHt96EMx1o"
      },
      "source": [
        "#Training and Evaluating the Agent in a Simulation Loop\n",
        "Before we run the agent, we need to set up a few environment parameters. These are the bare minimum:\n",
        "* *slate_size* sets the size of the set of elements presented to the user;\n",
        "* *num_candidates* specifies the number of documents present in the document database at any given time;\n",
        "* *resample_documents* specifies whether the set of candidates should be resampled between time steps according to the document distribution (more on this in [later notebooks](RecSim_Developing_an_Environment.ipynb)).\n",
        "* finally, we set the random seed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWXOzVcZMvKk"
      },
      "source": [
        "seed = 0\n",
        "np.random.seed(seed)\n",
        "env_config = {\n",
        "  'num_candidates': 10,\n",
        "  'slate_size': 2,\n",
        "  'resample_documents': True,\n",
        "  'seed': seed,\n",
        "  }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SY3iY0tdNOGm"
      },
      "source": [
        "Once we've created a dictionary of these, we can run training, specifying additionally the number of training steps, number of iterations and a directory in which to checkpoint the agent.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_b1LKyNONOVo"
      },
      "source": [
        "tmp_base_dir = '/tmp/recsim/'\n",
        "runner = runner_lib.TrainRunner(\n",
        "    base_dir=tmp_base_dir,\n",
        "    create_agent_fn=create_agent,\n",
        "    env=interest_evolution.create_environment(env_config),\n",
        "    episode_log_file=\"\",\n",
        "    max_training_steps=50,\n",
        "    num_iterations=10)\n",
        "runner.run_experiment()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFgCmk572rc8"
      },
      "source": [
        "After training is finished, we can run a separate simulation to evaluate the agent's performance. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVsxaPxfCEQx"
      },
      "source": [
        "  runner = runner_lib.EvalRunner(\n",
        "      base_dir=tmp_base_dir,\n",
        "      create_agent_fn=create_agent,\n",
        "      env=interest_evolution.create_environment(env_config),\n",
        "      max_eval_episodes=5,\n",
        "      test_mode=True)\n",
        "  runner.run_experiment()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ojxl_ZAX26wh"
      },
      "source": [
        "The cumulative reward across the training episodes will be stored in *base_dir/eval/*. However, RecSim also exports a more detailed set of summaries, including environment specific ones, that can be visualized in a Tensorboard. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "bFfrAEtpDQzD"
      },
      "source": [
        "#@title Tensorboard\n",
        "%tensorboard --logdir=/tmp/recsim/\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFLLAj6AjTvx"
      },
      "source": [
        "## References\n",
        "[SlateQ: A Tractable Decomposition for Reinforcement Learning with Recommendation Sets. IJCAI 2019: 2592-2599](https://arxiv.org/abs/1905.12767)"
      ]
    }
  ]
}