{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RecSim: Developing an Environment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZiminPark/recsim/blob/master/recsim/colab/RecSim_Developing_an_Environment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehxPDcc-SuPC"
      },
      "source": [
        "# Developing an Environment\n",
        "\n",
        "<p align=\"center\"><img width=\"70%\" src=\"https://github.com/google-research/recsim/blob/master/recsim/colab/figures/simulator.png?raw=true\" /></p>\n",
        "\n",
        "- 위 그림에서 초록색과 파란색 블락들이 RecSim 환경에서 구현해야하는 부분들이다.\n",
        "- 이번 노트에서는 각 블럭의 역할과 어떻게 얘네들이 조합되는지 살펴보자.\n",
        "- 이 과정에서 end-to-end로 구현해볼 것이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAZ4L-3Q4eme"
      },
      "source": [
        "# Overview\n",
        "\n",
        "RecSim simulation의 한 step은 다음처럼 요약될 수 있다.\n",
        "\n",
        "\n",
        "1.   document는 *D* documents를 추천시스템에 제공. 스텝마다 다를 수도 있고 시뮬레이션 기간동안 고정될 수도 있다. D는 a list of features 로 표현된다. fully observable 상황에서는 추천시스템이 유저의 state와 선택에 영향을 미치는 모든 feature를 볼 수 있다. 그러나 일반적으로 그럴 필요는 없을 것이다. <br>\n",
        "\n",
        "2.   Agent는 *D* documents와 마지막 추천에 대한 반응을 관측할 수 있다. 그리고 다시 *k* documents를 유저에게 추천한다. 순서는 유저 선택이나 state에 영향을 줄 수도 있고 아닐 수도 있다. 당신의 goal에 따라 다르다.<br>\n",
        "\n",
        "3. 유저는 추천된 documents목록을 보고 선택하거나 안 할 수도 있다. 이는 관측 유저의 반응과 latent space에 대한 반응을 만들어낸다. 보통 유저의 state가 fully 드러나지는 않는다.<br>\n",
        "\n",
        "위의 그림을 자세히 보면 정보가 acyclic함을 알 수 잇다. 즉 RecSim은 dynamic Bayesian network (DBN)이다. Box는 conditional probability distributions을 나타낸다. 이제 간단한 시뮬레이션 문제를 정의하고 실행할 것이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHNuO9HQ7O5_"
      },
      "source": [
        "#Simulation Scenario: Choc vs. Kale\n",
        "\n",
        "다음 상황을 생각해보자. Corpus의 요소들이 얼마나 영양가있는가로 결정된다. 영양가있는 아이템은 `kale`로 후회스러운 아이템은 `chocalate`로 부르자. 후회스러운 documents는 유저들이 많이 반응하지만 장기적으로는 유저의 만족도를 하락시킨다. 반대로 영양가 있는 아이템은 반응은 적지만 장기적으로는 높은 만족감을 만든다. 우리는 document 특징을 [0,1] 사이의 연속적인 feature로 모델할 거고 Kaleness 척도로 부르자. 1이 영양가 높은 거고 0이 후회스러운 것이다.\n",
        "\n",
        "유저의 latent state는 *satisfaction* 1차원 피쳐로 정의된다. \"kaley\"를 섭취하면 satisfaction이 증가하고 \n",
        "\"chocolate\"을 섭취하면 감소한다. 유저가 아이템을 소비하면 engagement 척도를 emit한다. 이 수치는 유저의 만족도에 비례하고 kaleness에 반비례한다.\n",
        "\n",
        "이제 우리의 목표는 장기적으로 유저의 engage가 높아지는 chocolatey and kaley 아이템 사이의 적절한 조합을 찾는 것이다.\n",
        "\n",
        "다양한 요소들에 대한 함수 형태를 살펴보자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "My8kjo8OWRnC"
      },
      "source": [
        "!pip install --upgrade --no-cache-dir recsim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a00rX0hWRMNl"
      },
      "source": [
        "import numpy as np\n",
        "from gym import spaces\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNKQb4XSFCXp"
      },
      "source": [
        "from recsim import document\n",
        "from recsim import user\n",
        "from recsim.choice_model import MultinomialLogitChoiceModel\n",
        "from recsim.simulator import environment\n",
        "from recsim.simulator import recsim_gym"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1NzyfWi7kUc"
      },
      "source": [
        "#A Document Model\n",
        "\n",
        "Document 클래스는 `recsim.document.AbstractDocument`를 상속받아 쓴다. \n",
        "\n",
        "base class를 상속받으면 `observation_space() `static method를 구현해야 한다. OpenAI gym의 `space` 타입으로 document observable features 형식을 반환한다. 그리고 realization of said space을 반환하는 `create_observation` 함수도 만들어야 한다.\n",
        "각각의 document는 unique integer ID를 가져야한다.\n",
        "\n",
        "우리의 경우 documents가 하나의 피쳐(kaleness value)만 갖는다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeBhRJrd7njP"
      },
      "source": [
        "class LTSDocument(document.AbstractDocument):\n",
        "  def __init__(self, doc_id, kaleness):\n",
        "    self.kaleness = kaleness\n",
        "    # doc_id is an integer representing the unique ID of this document\n",
        "    super(LTSDocument, self).__init__(doc_id)\n",
        "\n",
        "  def create_observation(self):\n",
        "    return np.array([self.kaleness])\n",
        "\n",
        "  @staticmethod\n",
        "  def observation_space():\n",
        "    return spaces.Box(shape=(1,), dtype=np.float32, low=0.0, high=1.0)\n",
        "  \n",
        "  def __str__(self):\n",
        "    return f\"Document {self._doc_id} with kaleness {self.kaleness}.\""
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-Ko0Adpxdjk"
      },
      "source": [
        "이제 document sampler를 만들어 보자. \n",
        "`document.AbstractDocumentSampler`를 상속 받아 쓰고 `sample_document()`함수가 구현되어야 한다. 특정 분포에서 샘플한 `document`를 반환해야 한다.\n",
        "우리의 경우 uniform distribution에서 추출할 것이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCOf-66UWZwe"
      },
      "source": [
        "class LTSDocumentSampler(document.AbstractDocumentSampler):\n",
        "  def __init__(self, doc_ctor=LTSDocument, **kwargs):\n",
        "    super(LTSDocumentSampler, self).__init__(doc_ctor, **kwargs)\n",
        "    self._doc_count = 0\n",
        "\n",
        "  def sample_document(self):\n",
        "    doc_features = {}\n",
        "    doc_features['doc_id'] = self._doc_count\n",
        "    doc_features['kaleness'] = self._rng.random_sample()\n",
        "    self._doc_count += 1\n",
        "    return self._doc_ctor(**doc_features)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i43PEB15y3LX"
      },
      "source": [
        "With this we can now simulate documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGPL7IcHyksr",
        "outputId": "701c5964-0cd4-4800-c579-f6ae1608555c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sampler = LTSDocumentSampler()\n",
        "for i in range(5): print(sampler.sample_document())\n",
        "d = sampler.sample_document()\n",
        "print(\"Documents have observation space:\", d.observation_space(), \"\\n\"\n",
        "      \"An example realization is: \", d.create_observation())"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Document 0 with kaleness 0.5488135039273248.\n",
            "Document 1 with kaleness 0.7151893663724195.\n",
            "Document 2 with kaleness 0.6027633760716439.\n",
            "Document 3 with kaleness 0.5448831829968969.\n",
            "Document 4 with kaleness 0.4236547993389047.\n",
            "Documents have observation space: Box(0.0, 1.0, (1,), float32) \n",
            "An example realization is:  [0.64589411]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TzJnr8F0gI4"
      },
      "source": [
        "RecSim에서는 직접 documents를 샘플하지 않고 시뮬레이터가 한다. 그래서 필요한 argument이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duPkmxSk7qen"
      },
      "source": [
        "#A User Model\n",
        "\n",
        "이제 유저 모델을 만들어 보자. 구성요소는 다음과 같다.\n",
        "* a user state,\n",
        "* a user sampler (a distribution over the starting state of users), \n",
        "* a user state transition model, and\n",
        "* a user response.\n",
        "\n",
        "튜토리얼에서 사용할 모델은 다음과 같다:\n",
        "* 각각의 유저는 called net kale exposure($\\text{nke}_t$)와 만족도($\\text{sat}_t$)를 갖고 있다. 둘은 satisfaction이 unbounded되어 있음을 표현하기 위해 logistic function으로 엮여있다. 즉, $$\\text{sat}_t = \\sigma(\\tau\\cdot\\text{nke}_t),$$ 이고 $\\tau$는 유저 고유의 민감도 파라미터이다. Satisfaction과 net kale exposure는 bijectively 서로 얽혀있다. 그래서 state를 추적하기 위해서는 한 가지만 저장하면 된다.<br>\n",
        "\n",
        "* slate $S$가 주어졌을 때 유저는 multinomial logit choice model을 기반으로 아이템을 선정한다. 이때 items' chocolateness를 피쳐로 쓴다. $p(\\text{user chooses }d_i \\text{ from slate }S) \\sim e^{1-\\mathrm{kaleness}(d_i)}$ 왜냐하면 초코리키한 걸 더 clicky하기 때문이다.\n",
        "\n",
        "* 유저가 document를 선택하면 net kale exposure은 다음처럼 변한다. $$\\text{nke}_{t+1} = \\beta \\cdot \\text{nke}_t + 2(k_d - 1/2) + {\\cal N}(0, \\eta),$$ \n",
        "$\\beta$는 유저 고유의 forgetting factor이고 $k_d$는 선택한 다큐의 kaleness, $\\eta$는 노이즈 분포의 표준편차다.\n",
        "\n",
        "* 마지막으로 유저가 선택한 콘텐츠를 $s_d$ 동안 사용했다. $s_d$는 $$s_d\\sim\\log{\\cal N}(k_d\\mu_k + (1-k_d)\\mu_c, k_d\\sigma_k + (1-k_d)\\sigma_c),$$ 에서 만들어졌다. 즉, log-normal distribution이고 파라미터로는 pure kale response $(\\mu_k, \\sigma_k)$와 pure choc response $(\\mu_c, \\sigma_c)$ 사이를 linearly interpolating값으로 사용한다.\n",
        "\n",
        "유저의 state는 $(\\text{sat}, \\tau, \\beta, \\eta, \\mu_k, \\sigma_k, \\mu_c, \\sigma_c).$로 결정된다. satisfaction 변수만 state중 변하는 값인 반면 다른 파라미터는 static하다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtFhtKrLSBiq"
      },
      "source": [
        "## User state and user sampler \n",
        "\n",
        "documents와 비슷하게 이런 파라미터의 container역할을 하는 user state class를 구현할 것이다. `AbstractDocument`와 비슷하게 `AbstractUserState`는 `observation_space()` 와 `create_observations()`을 구현해야 한다. 이런 애들은 유저 state의 부분적인(또는 모든) 정보를 매 Iter마다 agent에게 넘겨준다.\n",
        "\n",
        "시간 제한을 둘 수 있지만 요기서는 하지 않겠다.\n",
        "\n",
        "마지막으로 `score_document method`를 구현한다. document와 non-negative 실수를 대응하는 함수이다. 이 함수의 의의는 금방 알게될 것이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5hCeVFuq8qB"
      },
      "source": [
        "class LTSUserState(user.AbstractUserState):\n",
        "  def __init__(self, memory_discount, sensitivity, innovation_stddev,\n",
        "               choc_mean, choc_stddev, kale_mean, kale_stddev,\n",
        "               net_kaleness_exposure, time_budget, observation_noise_stddev=0.1\n",
        "              ):\n",
        "    ## Transition model parameters\n",
        "    ##############################\n",
        "    self.memory_discount = memory_discount\n",
        "    self.sensitivity = sensitivity\n",
        "    self.innovation_stddev = innovation_stddev\n",
        "\n",
        "    ## Engagement parameters\n",
        "    self.choc_mean = choc_mean\n",
        "    self.choc_stddev = choc_stddev\n",
        "    self.kale_mean = kale_mean\n",
        "    self.kale_stddev = kale_stddev\n",
        "\n",
        "    ## State variables\n",
        "    ##############################\n",
        "    self.net_kaleness_exposure = net_kaleness_exposure\n",
        "    self.satisfaction = 1 / (1 + np.exp(-sensitivity * net_kaleness_exposure))\n",
        "    self.time_budget = time_budget\n",
        "\n",
        "    # Noise\n",
        "    self._observation_noise = observation_noise_stddev\n",
        "\n",
        "  def create_observation(self):\n",
        "    \"\"\"User's state is not observable.\"\"\"\n",
        "    clip_low, clip_high = (-1.0 / (1.0 * self._observation_noise),\n",
        "                           1.0 / (1.0 * self._observation_noise))\n",
        "    noise = stats.truncnorm( \n",
        "        clip_low, clip_high, loc=0.0, scale=self._observation_noise).rvs()\n",
        "    noisy_sat = self.satisfaction + noise\n",
        "    return np.array([noisy_sat,])\n",
        "\n",
        "  @staticmethod\n",
        "  def observation_space():\n",
        "    return spaces.Box(shape=(1,), dtype=np.float32, low=-2.0, high=2.0)\n",
        "  \n",
        "  # scoring function for use in the choice model -- the user is more likely to\n",
        "  # click on more chocolatey content.\n",
        "  def score_document(self, doc_obs):\n",
        "    return 1 - doc_obs"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyUiJiKEq81y"
      },
      "source": [
        "- Document Model처럼 매 세션마다 starting 유저를 정의할 state sampler가 필요하다. \n",
        "\n",
        "- 이번 튜토리얼에서는 $\\text{nke}_0$를 샘플하고 나머지 파라미터는 같게 한다. 즉 우리는 다른 수준의 만족도를 갖는 같은 유저들을 다루는 것이다. 당연히 다른 파라미터를 갖는 다양한 랜덤 유저로 확장할 수도 있다.\n",
        "\n",
        "- $\\eta = 0$이면, $\\text{nke}$ 값은 $\\left[-\\frac{1}{1-\\beta}, \\ldots, \\frac{1}{1-\\beta} \\right]$에 항상 bound된다. 시작 분포는 이 구간에서 uniformly 샘플한다.\n",
        "샘플링 코드는 `sample_user()`에 구현한다. `user.AbstractUserSampler` base class의 메서드이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76xcc-7WZeAT"
      },
      "source": [
        "class LTSStaticUserSampler(user.AbstractUserSampler):\n",
        "  _state_parameters = None\n",
        "\n",
        "  def __init__(self,\n",
        "               user_ctor=LTSUserState,\n",
        "               memory_discount=0.9,\n",
        "               sensitivity=0.01,\n",
        "               innovation_stddev=0.05,\n",
        "               choc_mean=5.0,\n",
        "               choc_stddev=1.0,\n",
        "               kale_mean=4.0,\n",
        "               kale_stddev=1.0,\n",
        "               time_budget=60,\n",
        "               **kwargs):\n",
        "    self._state_parameters = {'memory_discount': memory_discount,\n",
        "                              'sensitivity': sensitivity,\n",
        "                              'innovation_stddev': innovation_stddev,\n",
        "                              'choc_mean': choc_mean,\n",
        "                              'choc_stddev': choc_stddev,\n",
        "                              'kale_mean': kale_mean,\n",
        "                              'kale_stddev': kale_stddev,\n",
        "                              'time_budget': time_budget\n",
        "                             }\n",
        "    super(LTSStaticUserSampler, self).__init__(user_ctor, **kwargs)\n",
        "\n",
        "  def sample_user(self):\n",
        "    starting_nke = ((self._rng.random_sample() - .5) *\n",
        "                    (1 / (1.0 - self._state_parameters['memory_discount'])))\n",
        "    self._state_parameters['net_kaleness_exposure'] = starting_nke\n",
        "    return self._user_ctor(**self._state_parameters)\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrXhPNf_cgEY"
      },
      "source": [
        "Let's try this out!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMMmFOcLchuX",
        "outputId": "f05c73fa-26c8-4352-db85-265e25f278fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "sampler = LTSStaticUserSampler()\n",
        "starting_nke = []\n",
        "for i in range(1000):\n",
        "  sampled_user = sampler.sample_user()\n",
        "  starting_nke.append(sampled_user.net_kaleness_exposure)\n",
        "_ = plt.hist(starting_nke)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ANKrwWKKRB1"
      },
      "source": [
        "## Response model\n",
        "\n",
        "- 다음으로 해야하는 건 `user response` class다. RecSim은 매 추천에 대해 반응을 생성한다. \n",
        "- 응답에 대한 내용은 agent가 추천에서 document-specific feedback을 볼 것이다. (non-document specific feedback은 `LTSUserState.create_observation`에서 생성된다.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTESuYu1J8-w"
      },
      "source": [
        "class LTSResponse(user.AbstractResponse):\n",
        "  # The maximum degree of engagement.\n",
        "  MAX_ENGAGEMENT_MAGNITUDE = 100.0\n",
        "\n",
        "  def __init__(self, clicked=False, engagement=0.0):\n",
        "    self.clicked = clicked\n",
        "    self.engagement = engagement\n",
        "\n",
        "  def create_observation(self):\n",
        "    return {'click': int(self.clicked), 'engagement': np.array(self.engagement)}\n",
        "\n",
        "  @classmethod\n",
        "  def response_space(cls):\n",
        "    # `engagement` feature range is [0, MAX_ENGAGEMENT_MAGNITUDE]\n",
        "    return spaces.Dict({\n",
        "        'click':\n",
        "            spaces.Discrete(2),\n",
        "        'engagement':\n",
        "            spaces.Box(\n",
        "                low=0.0,\n",
        "                high=cls.MAX_ENGAGEMENT_MAGNITUDE,\n",
        "                shape=tuple(),\n",
        "                dtype=np.float32)\n",
        "    })"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzKB9nX8si77"
      },
      "source": [
        "##User model\n",
        "\n",
        "이제 실제 유저의 행동을 구체화 해보자. RecSim user model은  `recsim.user.AbstractUserModel`을 base로 하고\n",
        "* user state을 유지하고,\n",
        "* 추천의 결과로 user state를 evolving하고\n",
        "* slate 추천의 결과에 대한 반응을 생성한다.\n",
        "\n",
        "끝에 가서는 user model은 `update_state()`와 `simulate_response()`을 구현해야 하고 세션의 끝을 알리는 `is_terminal`도 있어야 한다. self.time_budget*을 매 step 호출마다 줄이면 된다. 먼저 함수 단위로 나눠서 살펴보고 나중에 class로 합치자."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgovTg4s2Vbf"
      },
      "source": [
        "- init은 간단하다. response_model, user sampler, slate size을 base 모델인 AbstractUserModel의 생성자로 넘겨준다.\n",
        "\n",
        "- 다른 환경을 Exploring하면서 user model의 \\_\\_init\\_\\_ 함수가 simulation을 configuring할 때 많은 flexibility를 제공한다는 것을 볼 수 있다. 그러나 지금은 기본에 집중하자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HajPntS8GELb"
      },
      "source": [
        "def user_init(self,\n",
        "              slate_size,\n",
        "              seed=0):\n",
        "\n",
        "  super(LTSUserModel,\n",
        "        self).__init__(LTSResponse,\n",
        "                       LTSStaticUserSampler(LTSUserState,\n",
        "                                            seed=seed), slate_size)\n",
        "  self.choice_model = MultinomialLogitChoiceModel({})"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g01X7pcf4VT2"
      },
      "source": [
        "`simulate_response()`는 slate 추천을 받고 그에 대한 반응을 리턴한다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7j9jMGv0KvEu"
      },
      "source": [
        "def simulate_response(self, slate_documents):\n",
        "  # List of empty responses\n",
        "  responses = [self._response_model_ctor() for _ in slate_documents]\n",
        "  # Get click from of choice model.\n",
        "  self.choice_model.score_documents(\n",
        "    self._user_state, [doc.create_observation() for doc in slate_documents])\n",
        "  scores = self.choice_model.scores\n",
        "  selected_index = self.choice_model.choose_item()\n",
        "  # Populate clicked item.\n",
        "  self._generate_response(slate_documents[selected_index],\n",
        "                          responses[selected_index])\n",
        "  return responses\n",
        "\n",
        "def generate_response(self, doc, response):\n",
        "  response.clicked = True\n",
        "  # linear interpolation between choc and kale.\n",
        "  engagement_loc = (doc.kaleness * self._user_state.choc_mean\n",
        "                    + (1 - doc.kaleness) * self._user_state.kale_mean)\n",
        "  engagement_loc *= self._user_state.satisfaction\n",
        "  engagement_scale = (doc.kaleness * self._user_state.choc_stddev\n",
        "                      + ((1 - doc.kaleness)\n",
        "                          * self._user_state.kale_stddev))\n",
        "  log_engagement = np.random.normal(loc=engagement_loc,\n",
        "                                    scale=engagement_scale)\n",
        "  response.engagement = np.exp(log_engagement)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhSqHVbIebf0"
      },
      "source": [
        "`update_state()`메서드는 state transition kernel 을 구현해야 한다. 인풋으로 slate와 유저의 선택을 받는다. in-place로 바뀌어서 return은 없다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EQRVNW9KuT9"
      },
      "source": [
        "def update_state(self, slate_documents, responses):\n",
        "  for doc, response in zip(slate_documents, responses):\n",
        "    if response.clicked:\n",
        "      innovation = np.random.normal(scale=self._user_state.innovation_stddev)\n",
        "      net_kaleness_exposure = (self._user_state.memory_discount\n",
        "                                * self._user_state.net_kaleness_exposure\n",
        "                                - 2.0 * (doc.kaleness - 0.5)\n",
        "                                + innovation\n",
        "                              )\n",
        "      self._user_state.net_kaleness_exposure = net_kaleness_exposure\n",
        "      satisfaction = 1 / (1.0 + np.exp(-self._user_state.sensitivity\n",
        "                                        * net_kaleness_exposure)\n",
        "                          )\n",
        "      self._user_state.satisfaction = satisfaction\n",
        "      self._user_state.time_budget -= 1\n",
        "      return"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueKeYdpH5VGd"
      },
      "source": [
        "budget이 0이 되면 멈춘다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYztrGCdKrAF"
      },
      "source": [
        "def is_terminal(self):\n",
        "  \"\"\"Returns a boolean indicating if the session is over.\"\"\"\n",
        "  return self._user_state.time_budget <= 0"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IlApIga5brV"
      },
      "source": [
        "We have all the components to instantiate a user model, so let's wrap them up in a class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9g-Xso_5Ivx-"
      },
      "source": [
        "LTSUserModel = type(\"LTSUserModel\", (user.AbstractUserModel,),\n",
        "                    {\"__init__\": user_init,\n",
        "                     \"is_terminal\": is_terminal,\n",
        "                     \"update_state\": update_state,\n",
        "                     \"simulate_response\": simulate_response,\n",
        "                     \"_generate_response\": generate_response})"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgynvev0Jzgd"
      },
      "source": [
        " slate_size = 3\n",
        " num_candidates = 10\n",
        " ltsenv = environment.Environment(\n",
        "            LTSUserModel(slate_size),\n",
        "            LTSDocumentSampler(),\n",
        "            num_candidates,\n",
        "            slate_size,\n",
        "            resample_documents=True)\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moeAyn_8Iug0"
      },
      "source": [
        "## Recap\n",
        "Before we conclude, let's take a second to recap everything we've done so far. The diagram below maps all the classes we've implemented/imported from RecSim to the functional diagram of RecSim.\n",
        "![RecSim implementation](https://github.com/google-research/recsim/blob/master/recsim/colab/figures/simulator_implemented.png?raw=true)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlofIEOkJJ2P"
      },
      "source": [
        "## Interacting with an agent\n",
        "\n",
        "- 이제 환경에 관한 건 전부 구현했다. \n",
        "- agent를 학습/ 평가를 위해 먼저 반응을 실수로 대응시키는 reward function을 정하자. \n",
        "- 클릭한 documents의 engagement을 최대화하는 문제를 가정하자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5GhbiI2JJWO"
      },
      "source": [
        "def clicked_engagement_reward(responses):\n",
        "  reward = 0.0\n",
        "  for response in responses:\n",
        "    if response.clicked:\n",
        "      reward += response.engagement\n",
        "  return reward"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1FEWpgASVyy"
      },
      "source": [
        "이제 step단위로 익숙한 OpenAI gym wrapper를 사용할 것이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysMbbgh_JqsO"
      },
      "source": [
        "lts_gym_env = recsim_gym.RecSimGymEnv(ltsenv, clicked_engagement_reward)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_9yA0p2KwHf",
        "outputId": "d800dbe4-09f5-4399-abf3-92d3f4abd2d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "observation_0 = lts_gym_env.reset()\n",
        "print('Observation 0')\n",
        "print('Available documents')\n",
        "doc_strings = ['doc_id ' + key + \" kaleness \" + str(value) for key, value\n",
        "               in observation_0['doc'].items()]\n",
        "print('\\n'.join(doc_strings))\n",
        "print('Noisy user state observation')\n",
        "print(observation_0['user'])"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Observation 0\n",
            "Available documents\n",
            "doc_id 30 kaleness [0.26455561]\n",
            "doc_id 31 kaleness [0.77423369]\n",
            "doc_id 32 kaleness [0.45615033]\n",
            "doc_id 33 kaleness [0.56843395]\n",
            "doc_id 34 kaleness [0.0187898]\n",
            "doc_id 35 kaleness [0.6176355]\n",
            "doc_id 36 kaleness [0.61209572]\n",
            "doc_id 37 kaleness [0.616934]\n",
            "doc_id 38 kaleness [0.94374808]\n",
            "doc_id 39 kaleness [0.6818203]\n",
            "Noisy user state observation\n",
            "[0.64081684]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SX2WuQZ2iP1N",
        "outputId": "1aba4f3c-1f98-42df-f30f-ca7139cecb0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Agent recommends the first three documents.\n",
        "recommendation_slate_0 = [0, 1, 2]\n",
        "observation_1, reward, done, _ = lts_gym_env.step(recommendation_slate_0)\n",
        "print('Observation 1')\n",
        "print('Available documents')\n",
        "doc_strings = ['doc_id ' + key + \" kaleness \" + str(value) for key, value\n",
        "               in observation_1['doc'].items()]\n",
        "print('\\n'.join(doc_strings))\n",
        "rsp_strings = [str(response) for response in observation_1['response']]\n",
        "print('User responses to documents in the slate')\n",
        "print('\\n'.join(rsp_strings))\n",
        "print('Noisy user state observation')\n",
        "print(observation_1['user'])"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Observation 1\n",
            "Available documents\n",
            "doc_id 40 kaleness [0.3595079]\n",
            "doc_id 41 kaleness [0.43703195]\n",
            "doc_id 42 kaleness [0.6976312]\n",
            "doc_id 43 kaleness [0.06022547]\n",
            "doc_id 44 kaleness [0.66676672]\n",
            "doc_id 45 kaleness [0.67063787]\n",
            "doc_id 46 kaleness [0.21038256]\n",
            "doc_id 47 kaleness [0.1289263]\n",
            "doc_id 48 kaleness [0.31542835]\n",
            "doc_id 49 kaleness [0.36371077]\n",
            "User responses to documents in the slate\n",
            "{'click': 1, 'engagement': array(3.80960019)}\n",
            "{'click': 0, 'engagement': array(0.)}\n",
            "{'click': 0, 'engagement': array(0.)}\n",
            "Noisy user state observation\n",
            "[0.52654626]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNcr8LdShvIS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}