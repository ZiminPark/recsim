{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RecSim: Developing an Environment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZiminPark/recsim/blob/master/recsim/colab/RecSim_Developing_an_Environment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehxPDcc-SuPC"
      },
      "source": [
        "# Developing an Environment\n",
        "\n",
        "<p align=\"center\"><img width=\"70%\" src=\"https://github.com/google-research/recsim/blob/master/recsim/colab/figures/simulator.png?raw=true\" /></p>\n",
        "\n",
        "- 위 그림에서 초록색과 파란색 블락들이 RecSim 환경에서 구현해야하는 부분들이다.\n",
        "- 이번 노트에서는 각 블럭의 역할과 어떻게 얘네들이 조합되는지 살펴보자.\n",
        "- 이 과정에서 end-to-end로 구현해볼 것이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAZ4L-3Q4eme"
      },
      "source": [
        "# Overview\n",
        "\n",
        "RecSim simulation의 한 step은 다음처럼 요약될 수 있다.\n",
        "\n",
        "\n",
        "1.   document는 *D* documents를 추천시스템에 제공. 스텝마다 다를 수도 있고 시뮬레이션 기간동안 고정될 수도 있다. D는 a list of features 로 표현된다. fully observable 상황에서는 추천시스템이 유저의 state와 선택에 영향을 미치는 모든 feature를 볼 수 있다. 그러나 일반적으로 그럴 필요는 없을 것이다. <br>\n",
        "\n",
        "2.   Agent는 *D* documents와 마지막 추천에 대한 반응을 관측할 수 있다. 그리고 다시 *k* documents를 유저에게 추천한다. 순서는 유저 선택이나 state에 영향을 줄 수도 있고 아닐 수도 있다. 당신의 goal에 따라 다르다.<br>\n",
        "\n",
        "3. 유저는 추천된 documents목록을 보고 선택하거나 안 할 수도 있다. 이는 관측 유저의 반응과 latent space에 대한 반응을 만들어낸다. 보통 유저의 state가 fully 드러나지는 않는다.<br>\n",
        "\n",
        "위의 그림을 자세히 보면 정보가 acyclic함을 알 수 잇다. 즉 RecSim은 dynamic Bayesian network (DBN)이다. Box는 conditional probability distributions을 나타낸다. 이제 간단한 시뮬레이션 문제를 정의하고 실행할 것이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHNuO9HQ7O5_"
      },
      "source": [
        "#Simulation Scenario: Choc vs. Kale\n",
        "\n",
        "다음 상황을 생각해보자. Corpus의 요소들이 얼마나 영양가있는가로 결정된다. 영양가있는 아이템은 `kale`로 후회스러운 아이템은 `chocalate`로 부르자. 후회스러운 documents는 유저들이 많이 반응하지만 장기적으로는 유저의 만족도를 하락시킨다. 반대로 영양가 있는 아이템은 반응은 적지만 장기적으로는 높은 만족감을 만든다. 우리는 document 특징을 [0,1] 사이의 연속적인 feature로 모델할 거고 Kaleness 척도로 부르자. 1이 영양가 높은 거고 0이 후회스러운 것이다.\n",
        "\n",
        "유저의 latent state는 *satisfaction* 1차원 피쳐로 정의된다. \"kaley\"를 섭취하면 satisfaction이 증가하고 \n",
        "\"chocolate\"을 섭취하면 감소한다. 유저가 아이템을 소비하면 engagement 척도를 emit한다. 이 수치는 유저의 만족도에 비례하고 kaleness에 반비례한다.\n",
        "\n",
        "이제 우리의 목표는 장기적으로 유저의 engage가 높아지는 chocolatey and kaley 아이템 사이의 적절한 조합을 찾는 것이다.\n",
        "\n",
        "다양한 요소들에 대한 함수 형태를 살펴보자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "My8kjo8OWRnC"
      },
      "source": [
        "!pip install --upgrade --no-cache-dir recsim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a00rX0hWRMNl"
      },
      "source": [
        "import numpy as np\n",
        "from gym import spaces\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNKQb4XSFCXp"
      },
      "source": [
        "from recsim import document\n",
        "from recsim import user\n",
        "from recsim.choice_model import MultinomialLogitChoiceModel\n",
        "from recsim.simulator import environment\n",
        "from recsim.simulator import recsim_gym"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1NzyfWi7kUc"
      },
      "source": [
        "#A Document Model\n",
        "\n",
        "Document 클래스는 `recsim.document.AbstractDocument`를 상속받아 쓴다. \n",
        "\n",
        "base class를 상속받으면 `observation_space() `static method를 구현해야 한다. OpenAI gym의 `space` 타입으로 document observable features 형식을 반환한다. 그리고 realization of said space을 반환하는 `create_observation` 함수도 만들어야 한다.\n",
        "각각의 document는 unique integer ID를 가져야한다.\n",
        "\n",
        "우리의 경우 documents가 하나의 피쳐(kaleness value)만 갖는다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeBhRJrd7njP"
      },
      "source": [
        "class LTSDocument(document.AbstractDocument):\n",
        "  def __init__(self, doc_id, kaleness):\n",
        "    self.kaleness = kaleness\n",
        "    # doc_id is an integer representing the unique ID of this document\n",
        "    super(LTSDocument, self).__init__(doc_id)\n",
        "\n",
        "  def create_observation(self):\n",
        "    return np.array([self.kaleness])\n",
        "\n",
        "  @staticmethod\n",
        "  def observation_space():\n",
        "    return spaces.Box(shape=(1,), dtype=np.float32, low=0.0, high=1.0)\n",
        "  \n",
        "  def __str__(self):\n",
        "    return f\"Document {self._doc_id} with kaleness {self.kaleness}.\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-Ko0Adpxdjk"
      },
      "source": [
        "이제 document sampler를 만들어 보자. \n",
        "`document.AbstractDocumentSampler`를 상속 받아 쓰고 `sample_document()`함수가 구현되어야 한다. 특정 분포에서 샘플한 `document`를 반환해야 한다.\n",
        "우리의 경우 uniform distribution에서 추출할 것이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCOf-66UWZwe"
      },
      "source": [
        "class LTSDocumentSampler(document.AbstractDocumentSampler):\n",
        "  def __init__(self, doc_ctor=LTSDocument, **kwargs):\n",
        "    super(LTSDocumentSampler, self).__init__(doc_ctor, **kwargs)\n",
        "    self._doc_count = 0\n",
        "\n",
        "  def sample_document(self):\n",
        "    doc_features = {}\n",
        "    doc_features['doc_id'] = self._doc_count\n",
        "    doc_features['kaleness'] = self._rng.random_sample()\n",
        "    self._doc_count += 1\n",
        "    return self._doc_ctor(**doc_features)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i43PEB15y3LX"
      },
      "source": [
        "With this we can now simulate documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGPL7IcHyksr",
        "outputId": "d8912179-f1fe-4a1e-f74f-8769ab869cfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sampler = LTSDocumentSampler()\n",
        "for i in range(5): print(sampler.sample_document())\n",
        "d = sampler.sample_document()\n",
        "print(\"Documents have observation space:\", d.observation_space(), \"\\n\"\n",
        "      \"An example realization is: \", d.create_observation())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Document 0 with kaleness 0.5488135039273248.\n",
            "Document 1 with kaleness 0.7151893663724195.\n",
            "Document 2 with kaleness 0.6027633760716439.\n",
            "Document 3 with kaleness 0.5448831829968969.\n",
            "Document 4 with kaleness 0.4236547993389047.\n",
            "Documents have observation space: Box(0.0, 1.0, (1,), float32) \n",
            "An example realization is:  [0.64589411]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TzJnr8F0gI4"
      },
      "source": [
        "RecSim에서는 직접 documents를 샘플하지 않고 시뮬레이터가 한다. 그래서 필요한 argument이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duPkmxSk7qen"
      },
      "source": [
        "#A User Model\n",
        "\n",
        "이제 유저 모델을 만들어 보자. 구성요소는 다음과 같다.\n",
        "* a user state,\n",
        "* a user sampler (a distribution over the starting state of users), \n",
        "* a user state transition model, and\n",
        "* a user response.\n",
        "\n",
        "튜토리얼에서 사용할 모델은 다음과 같다:\n",
        "* 각각의 유저는 called net kale exposure($\\text{nke}_t$)와 만족도($\\text{sat}_t$)를 갖고 있다. 둘은 satisfaction이 unbounded되어 있음을 표현하기 위해 logistic function으로 엮여있다. 즉, $$\\text{sat}_t = \\sigma(\\tau\\cdot\\text{nke}_t),$$ 이고 $\\tau$는 유저 고유의 민감도 파라미터이다. Satisfaction과 net kale exposure는 bijectively 서로 얽혀있다. 그래서 state를 추적하기 위해서는 한 가지만 저장하면 된다.<br>\n",
        "\n",
        "* slate $S$가 주어졌을 때 유저는 multinomial logit choice model을 기반으로 아이템을 선정한다. 이때 items' chocolateness를 피쳐로 쓴다. $p(\\text{user chooses }d_i \\text{ from slate }S) \\sim e^{1-\\mathrm{kaleness}(d_i)}$ 왜냐하면 초코리키한 걸 더 clicky하기 때문이다.\n",
        "\n",
        "* 유저가 document를 선택하면 net kale exposure은 다음처럼 변한다. $$\\text{nke}_{t+1} = \\beta \\cdot \\text{nke}_t + 2(k_d - 1/2) + {\\cal N}(0, \\eta),$$ \n",
        "$\\beta$는 유저 고유의 forgetting factor이고 $k_d$는 선택한 다큐의 kaleness, $\\eta$는 노이즈 분포의 표준편차다.\n",
        "\n",
        "* 마지막으로 유저가 선택한 콘텐츠를 $s_d$ 동안 사용했다. $s_d$는 $$s_d\\sim\\log{\\cal N}(k_d\\mu_k + (1-k_d)\\mu_c, k_d\\sigma_k + (1-k_d)\\sigma_c),$$ 에서 만들어졌다. 즉, log-normal distribution이고 파라미터로는 pure kale response $(\\mu_k, \\sigma_k)$와 pure choc response $(\\mu_c, \\sigma_c)$ 사이를 linearly interpolating값으로 사용한다.\n",
        "\n",
        "############## 여기까지\n",
        "즉, 로그\n",
        "\n",
        "i.e. a log-normal distribution with parameters linearly interpolating between the pure kale response $(\\mu_k, \\sigma_k)$ and the pure choc response $(\\mu_c, \\sigma_c)$.\n",
        "\n",
        "Thus, a user state is defined by the tuple $(\\text{sat}, \\tau, \\beta, \\eta, \\mu_k, \\sigma_k, \\mu_c, \\sigma_c).$ The satisfaction variable is the only dynamic part of the state, whereas the other parameters define the user and are static. Technically, we are not required to keep these as part of the state as opposed to hardcoding them, however, it enables us to, say, sample users with different properties.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtFhtKrLSBiq"
      },
      "source": [
        "## User state and user sampler \n",
        "Similarly to documents, we begin by implementing a user state class, i.e. a class that acts as a container for all these parameters. Similarly to AbstractDocument, AbstractUserState requires us to implement an observation_space() and create_observations(). These are used to feed partial (or complete) information about the user's state to the agent at every iteration. \n",
        "\n",
        "We also maintain a time budget, which will cap the session length. In this scenario, the session length will be fixed to some constant, so it's not worth being explicit with our time budget modeling, but one can definitely consider this as part of the state and do more interesting thigs with it. \n",
        "\n",
        "Finally we will implement a score_document method, that maps a document to a non-negative real number. This significance of this will become clear shortly. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5hCeVFuq8qB"
      },
      "source": [
        "class LTSUserState(user.AbstractUserState):\n",
        "  def __init__(self, memory_discount, sensitivity, innovation_stddev,\n",
        "               choc_mean, choc_stddev, kale_mean, kale_stddev,\n",
        "               net_kaleness_exposure, time_budget, observation_noise_stddev=0.1\n",
        "              ):\n",
        "    ## Transition model parameters\n",
        "    ##############################\n",
        "    self.memory_discount = memory_discount\n",
        "    self.sensitivity = sensitivity\n",
        "    self.innovation_stddev = innovation_stddev\n",
        "\n",
        "    ## Engagement parameters\n",
        "    self.choc_mean = choc_mean\n",
        "    self.choc_stddev = choc_stddev\n",
        "    self.kale_mean = kale_mean\n",
        "    self.kale_stddev = kale_stddev\n",
        "\n",
        "    ## State variables\n",
        "    ##############################\n",
        "    self.net_kaleness_exposure = net_kaleness_exposure\n",
        "    self.satisfaction = 1 / (1 + np.exp(-sensitivity * net_kaleness_exposure))\n",
        "    self.time_budget = time_budget\n",
        "\n",
        "    # Noise\n",
        "    self._observation_noise = observation_noise_stddev\n",
        "\n",
        "  def create_observation(self):\n",
        "    \"\"\"User's state is not observable.\"\"\"\n",
        "    clip_low, clip_high = (-1.0 / (1.0 * self._observation_noise),\n",
        "                           1.0 / (1.0 * self._observation_noise))\n",
        "    noise = stats.truncnorm(\n",
        "        clip_low, clip_high, loc=0.0, scale=self._observation_noise).rvs()\n",
        "    noisy_sat = self.satisfaction + noise\n",
        "    return np.array([noisy_sat,])\n",
        "\n",
        "  @staticmethod\n",
        "  def observation_space():\n",
        "    return spaces.Box(shape=(1,), dtype=np.float32, low=-2.0, high=2.0)\n",
        "  \n",
        "  # scoring function for use in the choice model -- the user is more likely to\n",
        "  # click on more chocolatey content.\n",
        "  def score_document(self, doc_obs):\n",
        "    return 1 - doc_obs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyUiJiKEq81y"
      },
      "source": [
        "Also similarly to our document model, we have need a starting state sampler, that sets the starting user state for every session. For this tutoral, we will just sample the starting $\\text{nke}_0$ and keep all the static parameters the same, meaning that we essentially deal with the same user at different levels of satisfaction. One can, of course, extend this easily to generate also users with different parameters by randomizing the values. \n",
        "\n",
        "Observe that if $\\eta = 0$, $\\text{nke}$ would be bounded in the interval $\\left[-\\frac{1}{1-\\beta}, \\ldots, \\frac{1}{1-\\beta} \\right]$ at all times, so as starting distribution we just sample uniformly from that range. Sampling code has to be implemented in sample_user(), as required by the base class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76xcc-7WZeAT"
      },
      "source": [
        "class LTSStaticUserSampler(user.AbstractUserSampler):\n",
        "  _state_parameters = None\n",
        "\n",
        "  def __init__(self,\n",
        "               user_ctor=LTSUserState,\n",
        "               memory_discount=0.9,\n",
        "               sensitivity=0.01,\n",
        "               innovation_stddev=0.05,\n",
        "               choc_mean=5.0,\n",
        "               choc_stddev=1.0,\n",
        "               kale_mean=4.0,\n",
        "               kale_stddev=1.0,\n",
        "               time_budget=60,\n",
        "               **kwargs):\n",
        "    self._state_parameters = {'memory_discount': memory_discount,\n",
        "                              'sensitivity': sensitivity,\n",
        "                              'innovation_stddev': innovation_stddev,\n",
        "                              'choc_mean': choc_mean,\n",
        "                              'choc_stddev': choc_stddev,\n",
        "                              'kale_mean': kale_mean,\n",
        "                              'kale_stddev': kale_stddev,\n",
        "                              'time_budget': time_budget\n",
        "                             }\n",
        "    super(LTSStaticUserSampler, self).__init__(user_ctor, **kwargs)\n",
        "\n",
        "  def sample_user(self):\n",
        "    starting_nke = ((self._rng.random_sample() - .5) *\n",
        "                    (1 / (1.0 - self._state_parameters['memory_discount'])))\n",
        "    self._state_parameters['net_kaleness_exposure'] = starting_nke\n",
        "    return self._user_ctor(**self._state_parameters)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrXhPNf_cgEY"
      },
      "source": [
        "Let's try this out!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMMmFOcLchuX",
        "outputId": "a9438e01-4350-4f02-e2aa-e59114eadd00",
        "colab": {
          "height": 268
        }
      },
      "source": [
        "sampler = LTSStaticUserSampler()\n",
        "starting_nke = []\n",
        "for i in range(1000):\n",
        "  sampled_user = sampler.sample_user()\n",
        "  starting_nke.append(sampled_user.net_kaleness_exposure)\n",
        "_ = plt.hist(starting_nke)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0AOAcYQeAIwj9ABgHKEHAOP+F2Es/zpZogsHAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ANKrwWKKRB1"
      },
      "source": [
        "## Response model\n",
        "\n",
        "The next thing we want to check off our list is the user response class. RecSim will generate one response for every recommended item in the slate. The contents of the response are what the agent will see as document-specific feedback from the recommendation (the non-document specific feedback being generated in LTSUserState.create_observation).  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTESuYu1J8-w"
      },
      "source": [
        "class LTSResponse(user.AbstractResponse):\n",
        "  # The maximum degree of engagement.\n",
        "  MAX_ENGAGEMENT_MAGNITUDE = 100.0\n",
        "\n",
        "  def __init__(self, clicked=False, engagement=0.0):\n",
        "    self.clicked = clicked\n",
        "    self.engagement = engagement\n",
        "\n",
        "  def create_observation(self):\n",
        "    return {'click': int(self.clicked), 'engagement': np.array(self.engagement)}\n",
        "\n",
        "  @classmethod\n",
        "  def response_space(cls):\n",
        "    # `engagement` feature range is [0, MAX_ENGAGEMENT_MAGNITUDE]\n",
        "    return spaces.Dict({\n",
        "        'click':\n",
        "            spaces.Discrete(2),\n",
        "        'engagement':\n",
        "            spaces.Box(\n",
        "                low=0.0,\n",
        "                high=cls.MAX_ENGAGEMENT_MAGNITUDE,\n",
        "                shape=tuple(),\n",
        "                dtype=np.float32)\n",
        "    })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzKB9nX8si77"
      },
      "source": [
        "##User model\n",
        "\n",
        "Now that we have a way to generate users for our sessions, need to specify the actual user behavior. A RecSim user model (deriving from recsim.user.AbstractUserModel) is responsible for \n",
        "* maintaining user state, \n",
        "* evolving user state as a result of recommendations,\n",
        "* generating a response to a slate of recommendations.\n",
        "\n",
        "To this end, our user model is required by the base class to implement update_state() and simulate_response(), as well as is_terminal, which indicates when the end of the session occurs. This is facilitated by decreasing *self.time_budget* on every step.To make presentation clearer, we will define each function separately, and then assemble them into a class.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgovTg4s2Vbf"
      },
      "source": [
        "Our init is simple---we just pass the response_model constructor, user sampler and slate size down to the AbstractUserModel base class. Exploring other environments, the reader might notice that user model __init__ functions do offer a lot of flexibility for configuring the simulation. For now, however, we stick to the basics and hardcode things."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HajPntS8GELb"
      },
      "source": [
        "def user_init(self,\n",
        "              slate_size,\n",
        "              seed=0):\n",
        "\n",
        "  super(LTSUserModel,\n",
        "        self).__init__(LTSResponse,\n",
        "                       LTSStaticUserSampler(LTSUserState,\n",
        "                                            seed=seed), slate_size)\n",
        "  self.choice_model = MultinomialLogitChoiceModel({})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g01X7pcf4VT2"
      },
      "source": [
        "The simulate_response() method takes in a slate (list) of recommended (i.e., produced by the agent) LTSDocuments and must output a slate of user responses. The *k*-th response in the slate of responses corresponds to the *k*-th document in the recommendation slate. In this case, we pick one document to click on based on our choice model, and produce an engagement value. We will let the responses to the unclicked documents be vacuous, however, one might use them in more subtle ways (e.g., recording whether the user inspected that document, etc.).  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7j9jMGv0KvEu"
      },
      "source": [
        "def simulate_response(self, slate_documents):\n",
        "  # List of empty responses\n",
        "  responses = [self._response_model_ctor() for _ in slate_documents]\n",
        "  # Get click from of choice model.\n",
        "  self.choice_model.score_documents(\n",
        "    self._user_state, [doc.create_observation() for doc in slate_documents])\n",
        "  scores = self.choice_model.scores\n",
        "  selected_index = self.choice_model.choose_item()\n",
        "  # Populate clicked item.\n",
        "  self._generate_response(slate_documents[selected_index],\n",
        "                          responses[selected_index])\n",
        "  return responses\n",
        "\n",
        "def generate_response(self, doc, response):\n",
        "  response.clicked = True\n",
        "  # linear interpolation between choc and kale.\n",
        "  engagement_loc = (doc.kaleness * self._user_state.choc_mean\n",
        "                    + (1 - doc.kaleness) * self._user_state.kale_mean)\n",
        "  engagement_loc *= self._user_state.satisfaction\n",
        "  engagement_scale = (doc.kaleness * self._user_state.choc_stddev\n",
        "                      + ((1 - doc.kaleness)\n",
        "                          * self._user_state.kale_stddev))\n",
        "  log_engagement = np.random.normal(loc=engagement_loc,\n",
        "                                    scale=engagement_scale)\n",
        "  response.engagement = np.exp(log_engagement)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KW0TwaN35ua"
      },
      "source": [
        "The update_state() method implements our state transition kernel. It consumes the recommended slate, as well as the actual choices (responses) to induce a state transition. The state is modified in-place, so the function does not return anything."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EQRVNW9KuT9"
      },
      "source": [
        "def update_state(self, slate_documents, responses):\n",
        "  for doc, response in zip(slate_documents, responses):\n",
        "    if response.clicked:\n",
        "      innovation = np.random.normal(scale=self._user_state.innovation_stddev)\n",
        "      net_kaleness_exposure = (self._user_state.memory_discount\n",
        "                                * self._user_state.net_kaleness_exposure\n",
        "                                - 2.0 * (doc.kaleness - 0.5)\n",
        "                                + innovation\n",
        "                              )\n",
        "      self._user_state.net_kaleness_exposure = net_kaleness_exposure\n",
        "      satisfaction = 1 / (1.0 + np.exp(-self._user_state.sensitivity\n",
        "                                        * net_kaleness_exposure)\n",
        "                          )\n",
        "      self._user_state.satisfaction = satisfaction\n",
        "      self._user_state.time_budget -= 1\n",
        "      return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueKeYdpH5VGd"
      },
      "source": [
        "Finally, the session expires when the time budget goes to 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYztrGCdKrAF"
      },
      "source": [
        "def is_terminal(self):\n",
        "  \"\"\"Returns a boolean indicating if the session is over.\"\"\"\n",
        "  return self._user_state.time_budget <= 0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IlApIga5brV"
      },
      "source": [
        "We have all the components to instantiate a user model, so let's wrap them up in a class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9g-Xso_5Ivx-"
      },
      "source": [
        "LTSUserModel = type(\"LTSUserModel\", (user.AbstractUserModel,),\n",
        "                    {\"__init__\": user_init,\n",
        "                     \"is_terminal\": is_terminal,\n",
        "                     \"update_state\": update_state,\n",
        "                     \"simulate_response\": simulate_response,\n",
        "                     \"_generate_response\": generate_response})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPNPWRhqJ0Q4"
      },
      "source": [
        "Finally, we assemble all components into an Environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgynvev0Jzgd"
      },
      "source": [
        " slate_size = 3\n",
        " num_candidates = 10\n",
        " ltsenv = environment.Environment(\n",
        "            LTSUserModel(slate_size),\n",
        "            LTSDocumentSampler(),\n",
        "            num_candidates,\n",
        "            slate_size,\n",
        "            resample_documents=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moeAyn_8Iug0"
      },
      "source": [
        "## Recap\n",
        "Before we conclude, let's take a second to recap everything we've done so far. The diagram below maps all the classes we've implemented/imported from RecSim to the functional diagram of RecSim.\n",
        "![RecSim implementation](https://github.com/google-research/recsim/blob/master/recsim/colab/figures/simulator_implemented.png?raw=true)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlofIEOkJJ2P"
      },
      "source": [
        "## Interacting with an agent\n",
        "\n",
        "We now have a fully implemented environment in hand. In order to train/evaluate agents in this environment, we first need to specify a reward function. In RecSim, a reward function maps a set of responses to a real number. Suppose we want to maximize the engagement of the clicked documents.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5GhbiI2JJWO"
      },
      "source": [
        "def clicked_engagement_reward(responses):\n",
        "  reward = 0.0\n",
        "  for response in responses:\n",
        "    if response.clicked:\n",
        "      reward += response.engagement\n",
        "  return reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1FEWpgASVyy"
      },
      "source": [
        "Now, we simply use the OpenAI gym wrapper, which essentially provides a familiar step-based API."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysMbbgh_JqsO"
      },
      "source": [
        "lts_gym_env = recsim_gym.RecSimGymEnv(ltsenv, clicked_engagement_reward)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_9yA0p2KwHf",
        "outputId": "e177b91c-a822-41d2-b781-7658c49ed435",
        "colab": {
          "height": 550
        }
      },
      "source": [
        "observation_0 = lts_gym_env.reset()\n",
        "print('Observation 0')\n",
        "print('Available documents')\n",
        "doc_strings = ['doc_id ' + key + \" kaleness \" + str(value) for key, value\n",
        "               in observation_0['doc'].items()]\n",
        "print('\\n'.join(doc_strings))\n",
        "print('Noisy user state observation')\n",
        "print(observation_0['user'])\n",
        "# Agent recommends the first three documents.\n",
        "recommendation_slate_0 = [0, 1, 2]\n",
        "observation_1, reward, done, _ = lts_gym_env.step(recommendation_slate_0)\n",
        "print('Observation 1')\n",
        "print('Available documents')\n",
        "doc_strings = ['doc_id ' + key + \" kaleness \" + str(value) for key, value\n",
        "               in observation_1['doc'].items()]\n",
        "print('\\n'.join(doc_strings))\n",
        "rsp_strings = [str(response) for response in observation_1['response']]\n",
        "print('User responses to documents in the slate')\n",
        "print('\\n'.join(rsp_strings))\n",
        "print('Noisy user state observation')\n",
        "print(observation_1['user'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Observation 0\n",
            "Available documents\n",
            "doc_id 10 kaleness [0.79172504]\n",
            "doc_id 11 kaleness [0.52889492]\n",
            "doc_id 12 kaleness [0.56804456]\n",
            "doc_id 13 kaleness [0.92559664]\n",
            "doc_id 14 kaleness [0.07103606]\n",
            "doc_id 15 kaleness [0.0871293]\n",
            "doc_id 16 kaleness [0.0202184]\n",
            "doc_id 17 kaleness [0.83261985]\n",
            "doc_id 18 kaleness [0.77815675]\n",
            "doc_id 19 kaleness [0.87001215]\n",
            "Noisy user state observation\n",
            "[0.41857747]\n",
            "Observation 1\n",
            "Available documents\n",
            "doc_id 20 kaleness [0.97861834]\n",
            "doc_id 21 kaleness [0.79915856]\n",
            "doc_id 22 kaleness [0.46147936]\n",
            "doc_id 23 kaleness [0.78052918]\n",
            "doc_id 24 kaleness [0.11827443]\n",
            "doc_id 25 kaleness [0.63992102]\n",
            "doc_id 26 kaleness [0.14335329]\n",
            "doc_id 27 kaleness [0.94466892]\n",
            "doc_id 28 kaleness [0.52184832]\n",
            "doc_id 29 kaleness [0.41466194]\n",
            "User responses to documents in the slate\n",
            "{'click': 0, 'engagement': 0.0}\n",
            "{'click': 0, 'engagement': 0.0}\n",
            "{'click': 1, 'engagement': 16.002030934389694}\n",
            "Noisy user state observation\n",
            "[0.47885928]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}